{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-K Filings: Data Preprocessing\n",
    "\n",
    "This notebook demonstrates how to preprocess 10-K filings by extracting sections, cleaning text, and preparing the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Add project root to path for importing local modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_preprocessor import FilingPreprocessor\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Raw Filings\n",
    "\n",
    "Let's load the 10-K filings that we downloaded in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filings\n",
    "filings_file = '../data/raw/10k_filings.pkl'\n",
    "\n",
    "if os.path.exists(filings_file):\n",
    "    filings_df = pd.read_pickle(filings_file)\n",
    "    print(f\"Loaded {len(filings_df)} filings from {filings_file}\")\n",
    "else:\n",
    "    print(f\"Error: File not found: {filings_file}\")\n",
    "    print(\"Please run the '1_data_extraction.ipynb' notebook first to download the filings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information\n",
    "print(f\"Filings for {filings_df['ticker'].nunique()} companies over {filings_df['filing_year'].nunique()} years\")\n",
    "print(f\"Companies: {', '.join(sorted(filings_df['ticker'].unique()))}\")\n",
    "print(f\"Years: {', '.join(map(str, sorted(filings_df['filing_year'].unique())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Preprocessor\n",
    "\n",
    "Now, we'll create an instance of the `FilingPreprocessor` class that we'll use to extract sections and clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor = FilingPreprocessor()\n",
    "\n",
    "# Display the available section patterns\n",
    "print(\"Available section patterns:\")\n",
    "for section_name, pattern in preprocessor.section_patterns.items():\n",
    "    print(f\"- {section_name}: {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Processing\n",
    "\n",
    "Before processing all filings, let's test the preprocessor on a single filing to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample filing\n",
    "sample_filing = filings_df.iloc[0]\n",
    "print(f\"Sample filing: {sample_filing['ticker']} from {sample_filing['filing_year']}\")\n",
    "\n",
    "# First, let's clean the HTML and extract plain text\n",
    "sample_text = preprocessor.clean_html(sample_filing['filing_html'])\n",
    "print(f\"\\nExtracted {len(sample_text)} characters of plain text\")\n",
    "print(f\"Sample of plain text: {sample_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's extract sections from the text\n",
    "sample_sections = preprocessor.extract_all_sections(sample_text)\n",
    "\n",
    "# Print the sections found and their lengths\n",
    "print(\"Extracted sections:\")\n",
    "for section_name, section_text in sample_sections.items():\n",
    "    if section_text:  # Only show non-empty sections\n",
    "        print(f\"- {section_name}: {len(section_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample section (e.g., Risk Factors)\n",
    "section_name = 'item_1a'  # Risk Factors\n",
    "if section_name in sample_sections and sample_sections[section_name]:\n",
    "    print(f\"Sample of {section_name} (Risk Factors):\")\n",
    "    print(f\"{sample_sections[section_name][:1000]}...\")\n",
    "else:\n",
    "    print(f\"Section {section_name} not found in the sample filing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Filings\n",
    "\n",
    "Now that we've tested the preprocessor on a sample filing, let's process all the filings to extract sections and clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all filings\n",
    "print(\"Processing all filings... This may take several minutes.\")\n",
    "processed_df = preprocessor.process_filings(\n",
    "    filings_df,\n",
    "    extract_sections=True,\n",
    "    clean_text=True\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(processed_df)} filings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Processed Data\n",
    "\n",
    "Let's explore the processed data to see what we've extracted and how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the processed DataFrame\n",
    "print(\"Columns in the processed DataFrame:\")\n",
    "\n",
    "# Group columns by type (original, section, cleaned)\n",
    "original_cols = [col for col in processed_df.columns if not col.startswith('section_') and not col.startswith('clean_')]\n",
    "section_cols = [col for col in processed_df.columns if col.startswith('section_') and not col.endswith('_chars')]\n",
    "count_cols = [col for col in processed_df.columns if col.endswith('_chars')]\n",
    "clean_cols = [col for col in processed_df.columns if col.startswith('clean_')]\n",
    "\n",
    "print(f\"\\nOriginal columns ({len(original_cols)}): {original_cols}\")\n",
    "print(f\"\\nSection columns ({len(section_cols)}): {section_cols}\")\n",
    "print(f\"\\nSection character count columns ({len(count_cols)}): {count_cols}\")\n",
    "print(f\"\\nCleaned text columns ({len(clean_cols)}): {clean_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check section extraction success\n",
    "section_stats = pd.DataFrame()\n",
    "\n",
    "for section_col in section_cols:\n",
    "    # Get section name without prefix\n",
    "    section_name = section_col.replace('section_', '')\n",
    "    \n",
    "    # Count non-empty sections\n",
    "    count_col = f\"section_{section_name}_chars\"\n",
    "    if count_col in processed_df.columns:\n",
    "        non_empty = (processed_df[count_col] > 0).sum()\n",
    "        pct_found = non_empty / len(processed_df) * 100\n",
    "        avg_length = processed_df[processed_df[count_col] > 0][count_col].mean()\n",
    "        \n",
    "        # Add to stats\n",
    "        section_stats.loc[section_name, 'count'] = non_empty\n",
    "        section_stats.loc[section_name, 'percentage'] = pct_found\n",
    "        section_stats.loc[section_name, 'avg_length'] = avg_length\n",
    "\n",
    "# Sort by percentage found (descending)\n",
    "section_stats = section_stats.sort_values('percentage', ascending=False)\n",
    "\n",
    "# Display stats\n",
    "section_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Section Extraction Success\n",
    "\n",
    "Let's visualize the section extraction success rate to see which sections were most commonly found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot section extraction success\n",
    "plt.figure(figsize=(14, 6))\n",
    "section_stats['percentage'].plot(kind='bar', color='steelblue')\n",
    "plt.title('Section Extraction Success Rate', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Section', fontsize=14)\n",
    "plt.ylabel('Percentage Found (%)', fontsize=14)\n",
    "plt.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% Threshold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Section Lengths by Company\n",
    "\n",
    "Let's compare the lengths of key sections across different companies to see if there are any notable differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key sections for comparison\n",
    "key_sections = ['item_1', 'item_1a', 'item_7', 'item_7a', 'item_8']\n",
    "key_section_cols = [f\"section_{section}_chars\" for section in key_sections if f\"section_{section}_chars\" in processed_df.columns]\n",
    "\n",
    "# Calculate average section lengths by company\n",
    "section_lengths_by_company = processed_df.groupby('ticker')[key_section_cols].mean().reset_index()\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "section_lengths_melted = pd.melt(\n",
    "    section_lengths_by_company,\n",
    "    id_vars='ticker',\n",
    "    value_vars=key_section_cols,\n",
    "    var_name='section',\n",
    "    value_name='avg_chars'\n",
    ")\n",
    "\n",
    "# Clean up section names for display\n",
    "section_lengths_melted['section'] = section_lengths_melted['section'].apply(lambda x: x.replace('section_', '').replace('_chars', ''))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='section', y='avg_chars', hue='ticker', data=section_lengths_melted)\n",
    "plt.title('Average Section Lengths by Company', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Section', fontsize=14)\n",
    "plt.ylabel('Average Length (characters)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Company')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Section Lengths Over Time\n",
    "\n",
    "Let's see if there are any trends in section lengths over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average section lengths by year\n",
    "section_lengths_by_year = processed_df.groupby('filing_year')[key_section_cols].mean().reset_index()\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "section_lengths_by_year_melted = pd.melt(\n",
    "    section_lengths_by_year,\n",
    "    id_vars='filing_year',\n",
    "    value_vars=key_section_cols,\n",
    "    var_name='section',\n",
    "    value_name='avg_chars'\n",
    ")\n",
    "\n",
    "# Clean up section names for display\n",
    "section_lengths_by_year_melted['section'] = section_lengths_by_year_melted['section'].apply(lambda x: x.replace('section_', '').replace('_chars', ''))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(x='filing_year', y='avg_chars', hue='section', style='section', marker='o', data=section_lengths_by_year_melted)\n",
    "plt.title('Average Section Lengths Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Filing Year', fontsize=14)\n",
    "plt.ylabel('Average Length (characters)', fontsize=14)\n",
    "plt.legend(title='Section')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of Cleaned Text\n",
    "\n",
    "Let's look at a sample of the cleaned text to see how it differs from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample filing with non-empty Risk Factors section\n",
    "sample_idx = processed_df[processed_df['section_item_1a_chars'] > 0].index[0]\n",
    "sample_processed = processed_df.loc[sample_idx]\n",
    "\n",
    "# Display information about the sample\n",
    "print(f\"Sample filing: {sample_processed['ticker']} from {sample_processed['filing_year']}\")\n",
    "\n",
    "# Compare original and cleaned text for Risk Factors section\n",
    "original_text = sample_processed['section_item_1a']\n",
    "cleaned_text = sample_processed['clean_item_1a'] if 'clean_item_1a' in sample_processed else ''\n",
    "\n",
    "print(f\"\\nOriginal text length: {len(original_text)} characters\")\n",
    "print(f\"Cleaned text length: {len(cleaned_text)} characters\")\n",
    "print(f\"\\nOriginal text sample: {original_text[:500]}...\")\n",
    "print(f\"\\nCleaned text sample: {cleaned_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Data\n",
    "\n",
    "Now that we have processed the filings, let's save the processed data for use in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the processed data directory if it doesn't exist\n",
    "if not os.path.exists('../data/processed'):\n",
    "    os.makedirs('../data/processed')\n",
    "\n",
    "# Save to pickle file\n",
    "output_file = '../data/processed/processed_filings.pkl'\n",
    "processed_df.to_pickle(output_file)\n",
    "print(f\"Saved processed data to {output_file}\")\n",
    "\n",
    "# Save section extraction stats\n",
    "stats_file = '../data/processed/section_extraction_stats.csv'\n",
    "section_stats.to_csv(stats_file)\n",
    "print(f\"Saved section extraction stats to {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook (`3_exploratory_analysis.ipynb`), we'll explore the processed filings to gain insights into the data, including:\n",
    "1. Overview of available companies and years\n",
    "2. Analysis of section lengths and content\n",
    "3. Comparison of sections across companies and time\n",
    "4. Initial text analysis and word frequency analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}