{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-K Filings: Text Analysis\n",
    "\n",
    "This notebook demonstrates how to perform text analysis on 10-K filings, including word frequency analysis, topic modeling, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "# Add project root to path for importing local modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.analysis.text_analysis import TextAnalyzer\n",
    "from src.visualization.basic_plots import create_wordcloud\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Processed Filings\n",
    "\n",
    "Let's load the 10-K filings that we preprocessed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed filings\n",
    "processed_file = '../data/processed/processed_filings.pkl'\n",
    "\n",
    "if os.path.exists(processed_file):\n",
    "    processed_df = pd.read_pickle(processed_file)\n",
    "    print(f\"Loaded {len(processed_df)} processed filings from {processed_file}\")\n",
    "else:\n",
    "    print(f\"Error: File not found: {processed_file}\")\n",
    "    print(\"Please run the '2_data_preprocessing.ipynb' notebook first to preprocess the filings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information\n",
    "print(f\"Filings for {processed_df['ticker'].nunique()} companies over {processed_df['filing_year'].nunique()} years\")\n",
    "print(f\"Companies: {', '.join(sorted(processed_df['ticker'].unique()))}\")\n",
    "print(f\"Years: {', '.join(map(str, sorted(processed_df['filing_year'].unique())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Text Analyzer\n",
    "\n",
    "Now, we'll create an instance of the `TextAnalyzer` class that we'll use to analyze the text content of the filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text analyzer\n",
    "analyzer = TextAnalyzer()\n",
    "\n",
    "# Display some of the financial domain-specific stopwords\n",
    "print(\"Sample of financial domain-specific stopwords:\")\n",
    "print(list(analyzer.financial_stopwords)[:20])\n",
    "\n",
    "# Display some risk-related terms\n",
    "print(\"\\nSample of risk-related terms:\")\n",
    "print(list(analyzer.risk_terms)[:20])\n",
    "\n",
    "# Display some positive business terms\n",
    "print(\"\\nSample of positive business terms:\")\n",
    "print(list(analyzer.positive_terms)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis\n",
    "\n",
    "Let's analyze the most common words in different sections of the 10-K filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key sections to analyze\n",
    "key_sections = ['item_1', 'item_1a', 'item_7', 'item_7a']\n",
    "section_names = {\n",
    "    'item_1': 'Business',\n",
    "    'item_1a': 'Risk Factors',\n",
    "    'item_7': 'Management Discussion & Analysis',\n",
    "    'item_7a': 'Market Risk Disclosures'\n",
    "}\n",
    "\n",
    "# Create a dictionary to store word frequencies for each section\n",
    "section_word_frequencies = {}\n",
    "\n",
    "# Analyze word frequencies for each section\n",
    "for section in key_sections:\n",
    "    section_col = f'section_{section}'\n",
    "    \n",
    "    if section_col in processed_df.columns:\n",
    "        # Combine text from all filings for this section\n",
    "        combined_text = ' '.join(processed_df[section_col].dropna().astype(str))\n",
    "        \n",
    "        # Get word frequencies\n",
    "        word_freq = analyzer.get_word_frequencies(combined_text, top_n=50, remove_stopwords=True)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        section_word_frequencies[section] = word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top words for each section\n",
    "for section, word_freq in section_word_frequencies.items():\n",
    "    if word_freq is not None and not word_freq.empty:\n",
    "        # Get top 20 words\n",
    "        top_words = word_freq.head(20)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='frequency', y='word', data=top_words, palette='viridis')\n",
    "        plt.title(f'Top 20 Words in {section_names.get(section, section)}', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Frequency', fontsize=14)\n",
    "        plt.ylabel('Word', fontsize=14)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds\n",
    "\n",
    "Let's create word clouds for each section to visualize the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each section\n",
    "for section, word_freq in section_word_frequencies.items():\n",
    "    if word_freq is not None and not word_freq.empty:\n",
    "        # Create a dictionary of word frequencies for the word cloud\n",
    "        word_freq_dict = dict(zip(word_freq['word'], word_freq['frequency']))\n",
    "        \n",
    "        # Choose colormap based on section\n",
    "        if section == 'item_1a':  # Risk Factors\n",
    "            colormap = 'Reds'\n",
    "        elif section == 'item_7':  # MD&A\n",
    "            colormap = 'Blues'\n",
    "        elif section == 'item_1':  # Business\n",
    "            colormap = 'Greens'\n",
    "        else:\n",
    "            colormap = 'viridis'\n",
    "        \n",
    "        # Create word cloud\n",
    "        fig, ax = create_wordcloud(\n",
    "            word_freq_dict,\n",
    "            title=f'Word Cloud for {section_names.get(section, section)}',\n",
    "            figsize=(12, 8),\n",
    "            colormap=colormap,\n",
    "            max_words=100\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company-Specific Word Analysis\n",
    "\n",
    "Let's compare word usage across different companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a section for company comparison\n",
    "comparison_section = 'item_1a'  # Risk Factors\n",
    "section_col = f'section_{comparison_section}'\n",
    "\n",
    "# Check if the section exists in the data\n",
    "if section_col in processed_df.columns:\n",
    "    # Get companies to compare\n",
    "    companies = sorted(processed_df['ticker'].unique())\n",
    "    \n",
    "    # Create a dictionary to store word frequencies for each company\n",
    "    company_word_frequencies = {}\n",
    "    \n",
    "    # Analyze word frequencies for each company\n",
    "    for company in companies:\n",
    "        # Get the most recent filing for this company\n",
    "        company_filings = processed_df[processed_df['ticker'] == company].sort_values('filing_date', ascending=False)\n",
    "        if not company_filings.empty:\n",
    "            latest_filing = company_filings.iloc[0]\n",
    "            section_text = latest_filing[section_col]\n",
    "            \n",
    "            if isinstance(section_text, str) and section_text:\n",
    "                # Get word frequencies\n",
    "                word_freq = analyzer.get_word_frequencies(section_text, top_n=30, remove_stopwords=True)\n",
    "                company_word_frequencies[company] = word_freq\n",
    "    \n",
    "    print(f\"Analyzed word frequencies for {len(company_word_frequencies)} companies in the {section_names.get(comparison_section, comparison_section)} section.\")\n",
    "else:\n",
    "    print(f\"Section '{comparison_section}' not found in the processed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compare word frequencies across companies\n",
    "def compare_word_frequencies(company1, company2, top_n=15):\n",
    "    if company1 not in company_word_frequencies or company2 not in company_word_frequencies:\n",
    "        print(f\"Word frequencies not available for {company1} or {company2}.\")\n",
    "        return\n",
    "    \n",
    "    # Get word frequencies\n",
    "    freq1 = company_word_frequencies[company1]\n",
    "    freq2 = company_word_frequencies[company2]\n",
    "    \n",
    "    # Merge the frequencies\n",
    "    merged = pd.merge(freq1, freq2, on='word', how='outer', suffixes=(f'_{company1}', f'_{company2}')).fillna(0)\n",
    "    \n",
    "    # Calculate the difference\n",
    "    merged['difference'] = merged[f'frequency_{company1}'] - merged[f'frequency_{company2}']\n",
    "    \n",
    "    # Sort by absolute difference\n",
    "    merged['abs_difference'] = merged['difference'].abs()\n",
    "    merged = merged.sort_values('abs_difference', ascending=False)\n",
    "    \n",
    "    # Get top differentiating words\n",
    "    top_diff = merged.head(top_n)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a horizontal bar chart\n",
    "    bars = plt.barh(top_diff['word'], top_diff['difference'])\n",
    "    \n",
    "    # Color bars based on which company uses the word more\n",
    "    for i, bar in enumerate(bars):\n",
    "        if top_diff.iloc[i]['difference'] > 0:\n",
    "            bar.set_color('steelblue')  # Company 1 uses more\n",
    "        else:\n",
    "            bar.set_color('firebrick')  # Company 2 uses more\n",
    "    \n",
    "    # Add a vertical line at x=0\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Difference in Word Frequency', fontsize=14)\n",
    "    plt.ylabel('Word', fontsize=14)\n",
    "    plt.title(f'Word Frequency Differences in {section_names.get(comparison_section, comparison_section)}: {company1} vs {company2}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend([f'More in {company1}', f'More in {company2}'], loc='lower right')\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare word frequencies between two companies\n",
    "if len(companies) >= 2:\n",
    "    compare_word_frequencies(companies[0], companies[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Let's use topic modeling to identify key themes in the 10-K filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a section for topic modeling\n",
    "topic_section = 'item_7'  # MD&A\n",
    "section_col = f'section_{topic_section}'\n",
    "\n",
    "# Check if the section exists in the data\n",
    "if section_col in processed_df.columns:\n",
    "    # Get non-empty sections\n",
    "    section_texts = processed_df[processed_df[f'section_{topic_section}_chars'] > 0][section_col].tolist()\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(section_texts) > 5:\n",
    "        # Extract topics\n",
    "        n_topics = 5  # Number of topics to extract\n",
    "        model, vectorizer, topic_words, doc_topic_matrix = analyzer.extract_topics(\n",
    "            section_texts,\n",
    "            n_topics=n_topics,\n",
    "            n_top_words=10,\n",
    "            method='lda'  # Latent Dirichlet Allocation\n",
    "        )\n",
    "        \n",
    "        print(f\"Extracted {n_topics} topics from {len(section_texts)} {section_names.get(topic_section, topic_section)} sections.\")\n",
    "    else:\n",
    "        print(f\"Not enough data for topic modeling. Found only {len(section_texts)} non-empty sections.\")\n",
    "else:\n",
    "    print(f\"Section '{topic_section}' not found in the processed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topics and top words\n",
    "if 'topic_words' in locals() and topic_words:\n",
    "    # Create a DataFrame to display the topics\n",
    "    topics_df = pd.DataFrame()\n",
    "    \n",
    "    for topic_idx, words in topic_words:\n",
    "        topics_df[f'Topic {topic_idx+1}'] = words\n",
    "    \n",
    "    # Display the topics\n",
    "    topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "if 'topic_words' in locals() and topic_words:\n",
    "    # Create a figure with subplots for each topic\n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(20, 4))\n",
    "    \n",
    "    # Create a word cloud for each topic\n",
    "    for topic_idx, words in topic_words:\n",
    "        # Create a dictionary of word importance\n",
    "        word_importance = {word: 1/(i+1) for i, word in enumerate(words)}\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            background_color='white',\n",
    "            width=400,\n",
    "            height=300,\n",
    "            colormap=f'Blues_{topic_idx+3}',\n",
    "            max_words=10\n",
    "        ).generate_from_frequencies(word_importance)\n",
    "        \n",
    "        # Add to subplot\n",
    "        ax = axes[topic_idx]\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title(f'Topic {topic_idx+1}', fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Topics in {section_names.get(topic_section, topic_section)}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Distribution by Company\n",
    "\n",
    "Let's analyze how the topics are distributed across different companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for topic distribution analysis\n",
    "if 'doc_topic_matrix' in locals() and doc_topic_matrix is not None:\n",
    "    # Get the filings that have non-empty MD&A sections\n",
    "    section_filings = processed_df[processed_df[f'section_{topic_section}_chars'] > 0].reset_index(drop=True)\n",
    "    \n",
    "    # Make sure we have the same number of filings as documents in the topic matrix\n",
    "    if len(section_filings) == doc_topic_matrix.shape[0]:\n",
    "        # Add topic distributions to the filings DataFrame\n",
    "        for topic_idx in range(n_topics):\n",
    "            section_filings[f'topic_{topic_idx+1}'] = doc_topic_matrix[:, topic_idx]\n",
    "        \n",
    "        # Calculate average topic distribution by company\n",
    "        topic_dist_by_company = section_filings.groupby('ticker')[[f'topic_{i+1}' for i in range(n_topics)]].mean()\n",
    "        \n",
    "        # Display the topic distribution\n",
    "        topic_dist_by_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution by company\n",
    "if 'topic_dist_by_company' in locals() and not topic_dist_by_company.empty:\n",
    "    # Convert to long format for plotting\n",
    "    topic_dist_long = topic_dist_by_company.reset_index().melt(\n",
    "        id_vars='ticker',\n",
    "        value_vars=[f'topic_{i+1}' for i in range(n_topics)],\n",
    "        var_name='topic',\n",
    "        value_name='proportion'\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='ticker', y='proportion', hue='topic', data=topic_dist_long)\n",
    "    plt.title(f'Topic Distribution by Company in {section_names.get(topic_section, topic_section)}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Company', fontsize=14)\n",
    "    plt.ylabel('Topic Proportion', fontsize=14)\n",
    "    plt.legend(title='Topic')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Text Analysis\n",
    "\n",
    "Let's perform a comprehensive text analysis on the filings using the `analyze_filings` method of the `TextAnalyzer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive text analysis\n",
    "print(\"Performing comprehensive text analysis... This may take a few minutes.\")\n",
    "text_metrics_df = analyzer.analyze_filings(processed_df, sections=key_sections)\n",
    "print(f\"Analysis complete. Generated metrics for {len(text_metrics_df)} filings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the metrics DataFrame\n",
    "text_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text metrics across companies\n",
    "if not text_metrics_df.empty:\n",
    "    # Select key metrics for comparison\n",
    "    comparative_metrics = analyzer.compare_filings(text_metrics_df, groupby='ticker', section='full_text')\n",
    "    \n",
    "    # Display the comparative metrics\n",
    "    comparative_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment trends\n",
    "if not text_metrics_df.empty:\n",
    "    # Plot sentiment trends\n",
    "    fig = analyzer.plot_sentiment_trends(text_metrics_df, groupby='filing_year', section='full_text')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Analysis Results\n",
    "\n",
    "Let's save the results of our text analysis for use in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the results directory if it doesn't exist\n",
    "if not os.path.exists('../data/results'):\n",
    "    os.makedirs('../data/results')\n",
    "\n",
    "# Save text metrics\n",
    "text_metrics_file = '../data/results/text_analysis.pkl'\n",
    "text_metrics_df.to_pickle(text_metrics_file)\n",
    "print(f\"Saved text analysis results to {text_metrics_file}\")\n",
    "\n",
    "# Save word frequencies\n",
    "word_freq_file = '../data/results/word_frequencies.pkl'\n",
    "pd.to_pickle(section_word_frequencies, word_freq_file)\n",
    "print(f\"Saved word frequencies to {word_freq_file}\")\n",
    "\n",
    "# Save topic modeling results if available\n",
    "if 'topic_words' in locals() and topic_words:\n",
    "    topic_file = '../data/results/topic_modeling.pkl'\n",
    "    topic_results = {\n",
    "        'section': topic_section,\n",
    "        'n_topics': n_topics,\n",
    "        'topic_words': topic_words,\n",
    "        'doc_topic_matrix': doc_topic_matrix\n",
    "    }\n",
    "    pd.to_pickle(topic_results, topic_file)\n",
    "    print(f\"Saved topic modeling results to {topic_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook (`5_financial_analysis.ipynb`), we'll extract and analyze financial metrics from the 10-K filings, including:\n",
    "1. Revenue, net income, and other key financial metrics\n",
    "2. Financial ratios and growth rates\n",
    "3. Comparative financial analysis across companies\n",
    "4. Financial trends over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}